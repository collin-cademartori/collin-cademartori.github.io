---
layout: post
mathjax: true
title: On Priors and Mechanisms 
---
I've been thinking recently about the standard Bayesian story about prior distributions, and I want to use this post to record some thoughts I have about that story. I take the "standard Bayesian story" about prior distributions to go something like this. You start with a sampling distribution $p(x| \theta)$. This sampling distribution might represent the measurement error of some device or the range of views in some population of people, for example. In any event, we usually tacitly take the sampling distribution to model something "real" in the world. Now if we're being Bayesian, we also need a prior distribution $p(\theta)$. Usually (though perhaps not always), it is appropriate to think that there is some true fixed value $\theta^*$ in the world. This is supposed to create to create an interpretive problem for the Bayesian. If we think that there is a true value of $\theta$, then although it is unknown, it is not random. So any non-degenerate probability distribution on $\theta$ must give us a misspecified model of the world!

Of course, you probably know how the Bayesian is supposed to get out of this trap. The Bayesian is supposed to model all uncertainty as randomness, whether it results from things like measurement error (which derives from a process in the world) or simple lack of information (which is an epistemic state within the Bayesian's head). Since the parameter $\theta$ is thought to be fixed in the world, the prior must represent uncertainty of latter, subjective form.

Now, we might ask how we should feel about this state of affairs. Maybe we think this "probability is subjective" business is a fantastic innovation that can solve all of our interpretive problems. On this view, we probably shouldn't even allow the above distinction I make between randomness in the world and in our heads. In the words of Mayor Pete, this becomes a distinction without a difference if all randomness should ultimately be cached out in epistemic terms. Furthermore, on this view, the prior and the sampling distribution should have the same status - both representing epistemic uncertainty, just about different quantities.

In describing such a view, I don't mean to say that I think any sizeable group of statisticians actually hold this view in practice. I mean this instead as a prototype of the kind of view that flows from taking the subjective view of probability at face value. Such views have the appealing feature of being easy to articulate and being internally consistent. However, such views can also lead to dramatic and dangerous consequences for statistical practice. For instance, suppose my model really does just represent my interal epistemic state about the quantities of interest. Then the posterior just tells me how I must change my beliefs once I observe data if I want to remain rational (assuming that following the probability axioms is necessary for rationality, as is often argued). If the inputs and outputs of the model are all just statements about my (rational) beliefs, how can data ever falsify any part of my model?

Indeed, suppose that I perform a model check such as a prior predictive check. If this check shows that my model generates data that looks pretty different from the observed data, then what should I do? If we are sticking to the belief interpretation of probability, then we might be tempted to say that I have found evidence that my beliefs are wrong. So I should update them in light of the evidence. But wait! The prior was supposed to represent my pre-data belief. So this whole prior predictive check thing seems to be missing the point. The posterior ditribution already tells me the unique way to update my beliefs with data in a rational way.

This idea, which has been called Bayesian confirmation theory, has been taken seriously by many. However, I take the fact that it at best makes model checks awkward to incorporate (sacrificing the virtue of self-consistency) or outright bans them to be a reductio of this viewpoint. It is simply plain to see that ignoring something like a failed prior predictive check could lead to very bad practical consequences down the road, especially if the prior is strong enough to constrain the posterior in non-trivial ways. In other words, while I see the philosophical appeal of this view (in fact I argued for it at length in a term paper for my first philosophy of science class!), I think that any working statistician must conclude that it founders on the rocky shoal of practice.

So suppose we try to go in the opposite direction. Instead of embracing subjectivity, we could try to minimize its role in our models and thereby assimilate ourselves to the frequentitsts. This line of thinking has loomed large in applied Bayesian thinking. In this direction, some Bayesians have pointed to the fact that the prior "washes out" as we acquire more data, and that the posterior becomes asymptotically normal under the right conditions (according to Berstein-von Mises). However, an applied statistician won't be satisfied with asymptotic reassurances. Indeed, the way this attitude has translated most directly into practice has been in the wide-ranging interest in non-informative priors.

Here the idea is that the subjective element of the Bayesian model is dangerous. Thus, we should include as little of our own subjective beliefs in the prior as possible. There has not been much agreement about how to cache out noninformativeness, but popular choices have been to select (often improper) uniform distributions, distributions that are invariant under transformations, and distributions that maximize some expected divergence between the prior and the posterior. The upshot is supposed to be that this approach allows the data to speak as unimpeded as possible by our subjective belief.

However, there are problems with this approach as well. For instance, Gabry et al. point out that any proper prior induces a proper prior data generating process $p(y)$. If we take a weakly informative prior such as $N(0,\sigma^2)$ with $\sigma$ large as an approximation to a uniform noninformative prior, then this prior data generating distribution could easily frequently yield values of $y$ that we know from even a crude familiarity with the domain are orders of magnitude too large to be plausible.

Gabry et al. argue that the prior should be chosen so that the induces data generating distribution $p(y)$ doesn't generate fake data that are obviously implausible to us. They say that this is a better notion of noninformative prior, but this idea bears little resemblance to the earlier notions of noninformativeness. Before, this was supposed to require that the prior not effect the posterior in any significiant way. But this new notion says that the prior should affect our inferences by keeping them in line with basic substantive knowledge about the domain of interest. Thus, it is not clear to me that such a prior is meaningfully noninformative. In fact, we might be able to construct many different priors with different levels of complexity and hierarchical structure all of which pass this test. It does not seem natural to me that these potentially complex and heterogenous priors should all be said to be noninformative. That said, this is just a semantic point. I merely want to point out that if such a prior is to be noninformative, it is in a sense that is a significant break with the notion of noninformativeness discussed above.

In fact, this discussion raises an even more basic point. This is simply that any (proper) prior implies a generative distribution $p(y)$. One such distribution more alignment with expectation or observed reality than another, but one such distribution simply cannot be more agnositic about the state of the world than another. In this sense, I think that the original notion of noninformativeness that seems to seek this kind of agnosticism is simply not practicable unless we willfully ignore certain consequences of our modeling choices.

So, where does this leave us? The hardcore subjectivist can't account for the importance of model checking, and the advocates of noninformative priors seem to ignore important consequences of their modelling choices. At this point, you may be asking what the point of all this pseudo-philosophical effort at interpretation is. Perhaps the experience of the applied statistician should suffice to inform them that prior predictive checks are important and that dogmatism shouldn't prevent us from creating better fitting models. Unfortunately, it seems this is not the case. Some have taken Bayesian confirmation theory very seriously, and the use of uninformative priors remains widespread. I do not inted to assign blame in making these remarks. I would only speculate that these confusions have resulted from either an over abundance of confidence or of fear regarding the so-called subjective element of modelling.

I want to spend the rest of this post sketching an alternate interpretation of the modelling process that avoids the problems mentioned above. This view is not my invention; it is at least implicit in much of hierarchical modelling. However, by articularing this articulation explicitly I hope to indicate that it can replace both of the above viewpoints and provide a satisfactory account of what we are saying about the world when we build Bayesian models.

If this view is to have a name, I would refer to it as the generative or pseudo-mechanical view of Bayesian modelling. On this account, the prior and the likelihood together can be viewed as a hypothesized data-generating process. In other words, we view the model itself as a scientific hypothesis about the way that the data are generated in the world. Importantly, this view doesn't make the usual ontological distinction between the data $y$ and the parameter $\theta$, where the former is drawn randomly but the latter is fixed but unknown. How can we justify this?

Although we usually should think that the true value of $\theta$ exists and is fixed, it is also usually natural to imagine that there is some process that generated $\theta$. If $\theta$ is the presidential approval rating, then it is certainly influenced by the state of the economy and whether or not we are at war, for instance. But as soon as we imagine a process that produced $\theta$, it is naturial to think that the process could have turned out differently than it did. This process could of course be hopelessly complex, but that's why we're statisticians! We can use probability distirbutions to capture the aggregate behavior of a system that we cannot model exactly. We can then use deterministic functional forms and dependence relations to capture those aspects of this deterministic process that we take to be most operative in determining the ultimate outcome $\theta$. This process is not really any different than the process of constructing a sampling distribution for $y$ given $\theta$, especially if the case of hierarchical models.

Thus, the overall model isn't a combination of a sampling model with a quantification of our subjective uncertainty about relevant parameters on this view. Instead, the model is a probabalistic approximation to some deterministic mechanism that generates the data along with the relevant parameters. Here, the data and the parameters do not differ in their basic ontological status in the model, only in whether or not they are observed.

This view has the attractive feature of neatly factoring or uncertainty in the following way. The uncertainty in the model, expressed by the various probability distributions we use, capture uncertainty induced by complexity in the underlying deterministic true data-generating process. Given the hypothesis that is the model, this uncertainty is not "in our heads" but rather describes aggregate behavior of systems in the world. On the other hand, uncertainty \textit{about} the model (i.e. how misspecified it is) is entirely subjective. It represents our lack of information about whether the true data generating process matched the one we've written down. Thus, we can handle and quantify these two types of uncertainty spearately - the first through the posterior distribution and the second through prior and posterior checks. Consequently, this view avoids the limitation of both of the above views.

It also avoids some of the awkward features of the above views. The vision of Bayesian statistics given to us by the advocate of noninformative priors is one in which priors are an annoying inconvenience we have to get beyond in order to get a posterior distribution. But since the posterior distribution is entirely determined by the joint distribution $p(y,\theta)$, it cannot really escape the conceptual taint of the prior. This is perhaps just a restatement of the above point that there is no truly agnostic prior. On the generative view, by contrast, the posterior can be explained neatly in terms of the prior model. If we consider a hypothetical infinite collecton of replications or simulations from our prior model and sampling distribution, then the posterior distribution is just the distribution describing those $\theta$ values for which the corresponding $y$ values looked like our actually observed data. In other words, the posterior distribution is the distribution of those $\theta$ values that could have generated our data given that the prior model and sampling distirbution are the true data generating process.

And by bringing together the concepts of uncertainty within versus about the model and objective objective versus subjective uncertainty, this view avoids the contortions that the subjectivist view underwent to try to capture all uncertainty within the model. As I stated above, I don't take this view to lead to any novel innovation in modelling techniques. But by articularing a single coherent way to think about the various stages of model building and checking, I hope that it might help attenuate some of the confusion about the status and role of these processes. At least, it doesn't succumb to the most egregious consequences of the most common dogmas of model building.

